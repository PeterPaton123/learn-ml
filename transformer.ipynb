{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\textbf{Task: Implement a transformer model from scratch}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivating Attention\n",
    "\n",
    "Recurrent architectures generate a series of internal states $h_{t}$ from the previous internal state $h_{t-1}$ and the value of the sequence at time $t$, this is inherently sequential and becomes a bottleneck for long sequences, such as large text corpus. Furthermore it struggles to model long range dependencies in the data, and relevance is dominanted by the recent past. The outputs of attention contains individual token information and contextual relationships between tokens too.\n",
    "\n",
    "Attention calculates a contextualised meaning of a token, which is beneficial as the same token can have different meanings in different contexts. For example:\n",
    "\"It's not **fair**\" vs \"He has **fair** hair\".\n",
    "\n",
    "### Attention Mechanism\n",
    "\n",
    "At the core of the transformer model is the attention mechanism, which allows for the modelling of dependencies without hinderance from distance apart in the sequence. \n",
    "\n",
    "Simply: **Attention aggregating information from all other tokens in the sequence, weighted by their relevance to the current token.**\n",
    "\n",
    "Essentially, the attention mechanism comprises of three matrices:\n",
    " - $Q \\in \\mathbb{R}^{NQ \\times d_{k}}$ the query matrix.\n",
    " - $K \\in \\mathbb{R}^{NK \\times d_{k}}$ the key matrix.\n",
    " - $V \\in \\mathbb{R}^{NK \\times d_{v}}$ the value matrix.\n",
    "\n",
    "With each key (row in key matrix), there is a corresponding value (row in the value matrix). The formula of attention is rather intuiative, considering a single query $\\vec{q}$ represented as row vector of length $d_{k}$, we calculate the dot-product between the query and every key in the key matix: $\\vec{q} \\cdot K^{T}$, therefore over a batch of $NQ$ queries assembled in a matrix $Q$ our pair-wise similarities is given by $Q\\cdot K^{T} \\in \\mathbb{R}^{NQ \\times NK}$. So, $\\left(Q\\cdot K^{T} \\right)_{ij}$ is the similarity between query $i$ and key $j$. \n",
    "\n",
    "We then normalise over the number of values we have to ensure our similarity values are of similar variance as to when they were inputted, mainly it helps for numerical stability of the softmax function and ensuring we have non-negligible gradients (If any of the values are significantly larger than others then it basically diminishes all others to 0 regardless of their true value). We softmax over each row of this similarity matrix, to get the normalised similarities, so the query-key similarities of each row sums to $1$. These are effectively weightings of the corresponding values in the value matrix $V$.\n",
    "\n",
    "$$ \\texttt{att}(Q, K, V) = \\texttt{softmax}\\left(\\frac{1}{\\sqrt{d_{k}}} Q \\cdot K^{T} \\right) V. $$\n",
    "\n",
    "For example, if a key and query are highly aligned, then the value normalised alignment will be close to $1$ and the resulting value will be similar to the value associated with the initial key.\n",
    "\n",
    "Sometimes, the softmax is decomposed into two steps, first we exponentiate all the elements and then we normalise each row. However, we can apply a mask to the weights before doing the softmax, zeroing some elements. e.g. if we make the masking matrix all ones below the diagonal, then we make the attention mechanism causal. That is, only queries appearing before the key can be used for attention. Our model cannot use future tokens in predicting the present.\n",
    "\n",
    "Finally, after the softmax, we can apply dropout. This effectively removes certain key query similarities, which could help our model by removing dominant key query similarities, so our model infers relationships between less similar key query pairs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The attention mechanism learns how to attend the information from one sequence to the information in another sequence. This is known as cross-attention. In the case that we want to learn about relationships within a sequence itself, these two sequences are the same, this is known as self-attention. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flax.linen as nn\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "\n",
    "from typing import Optional\n",
    "\n",
    "class AttentionMechanism(nn.Module):\n",
    "    d_key : int\n",
    "    d_value : int\n",
    "    dropout_rate : float = 0.0\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, \n",
    "        sequence : jnp.ndarray, \n",
    "        target_sequence : jnp.ndarray,\n",
    "        mask : Optional[jnp.ndarray] = None, \n",
    "        dropout_rng = None\n",
    "    ):\n",
    "        # We project our sequence to keys, queries and values with learned projections\n",
    "        query_matrix = nn.Dense(features=self.d_key, kernel_init=nn.initializers.lecun_normal())(target_sequence) # (NQ = len(target_sequence), d_key)\n",
    "        key_matrix   = nn.Dense(features=self.d_key, kernel_init=nn.initializers.lecun_normal())(sequence)     # (NK = len(sequence), d_key)\n",
    "        value_matrix = nn.Dense(features=self.d_value, kernel_init=nn.initializers.lecun_normal())(sequence) # (NK = len(sequence), d_value)\n",
    "\n",
    "        un_normalised_weights = jnp.exp(query_matrix @ key_matrix.T)\n",
    "        if (mask is not None):\n",
    "            un_normalised_weights = mask * un_normalised_weights\n",
    "        un_normalised_weights += 10e-5\n",
    "        weights = un_normalised_weights / jnp.sum(un_normalised_weights, axis = 0)\n",
    "        if self.dropout_rate > 0.0 and dropout_rng is not None:\n",
    "            dropout_mask = jax.random.bernoulli(dropout_rng, shape=weights.shape, p = 1 - self.dropout_rate)\n",
    "            weights *= dropout_mask / (1 - self.dropout_rate)\n",
    "        return weights @ value_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[-0.6498003 , -0.6035281 ,  0.1453905 ,  2.3583808 , -0.22132717,\n",
       "         0.10294747,  1.2881615 , -0.33101836, -0.62982154, -0.10315502],\n",
       "       [-0.6498003 , -0.6035281 ,  0.1453905 ,  2.3583808 , -0.22132717,\n",
       "         0.10294747,  1.2881615 , -0.33101836, -0.62982154, -0.10315502],\n",
       "       [-0.6498003 , -0.6035281 ,  0.1453905 ,  2.3583808 , -0.22132717,\n",
       "         0.10294747,  1.2881615 , -0.33101836, -0.62982154, -0.10315502]],      dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AttentionMechanism(d_key = 10, d_value=10)\n",
    "rng, init_rng = jax.random.split(jax.random.PRNGKey(42), 2)\n",
    "params = model.init(init_rng, jnp.ones((3, 4)), jnp.ones((3, 4)))\n",
    "model.apply(params, jnp.ones((3, 4)), jnp.ones((3, 4)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computational Cost of Attention\n",
    "\n",
    "As with most deep learning architectures, the computational cost of attention lies in the matrix multiplications. \n",
    "\n",
    "- Calculating the key matrix: $O(NK \\times d_{model} \\times d_{key})$\n",
    "- Calculating the value matrix: $O(NK \\times d_{model} \\times d_{value})$\n",
    "- Calculating the query matrix: $O(NQ \\times d_{model} \\times d_{key})$\n",
    "- $Q \\times K^{T},~~((NQ \\times d_{key}) \\cdot (NK \\times d_{key})^{T})$, which has a cost $O(NQ \\times d_{key} \\times NK)$.\n",
    "- $\\left( Q \\times K^{T} \\right) \\times V,~~ ((NQ \\times NK) \\cdot (NK \\times d_{value}))$, which has a cost $O(NQ \\times NK \\times d_{value})$.\n",
    "\n",
    "So the total computational cost is $O(NQ \\times NK \\times (d_{key} + d_{value}))$.\n",
    "\n",
    "The memory cost is the size of the key and value matrices, and the weight matrix: $O(NK \\times d_{key} + NK \\times d_{value} + NQ \\times d_{key} + NQ \\times NK)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Head Attention\n",
    "\n",
    "In practice, instead of having a single Attention head, the model can better learn different semantic and syntactic relationships by projecting the queries, keys and values to a lower dimension, and performing attention there, before concatenating the results and doing a final linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    # For multi head attention, d_key = d_value = d_model, which is then reduced in each attention head\n",
    "    d_model : int\n",
    "    num_heads : int\n",
    "    dropout_rate : float = 0.0\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self,\n",
    "        sequence : jnp.ndarray,\n",
    "        target_sequence : jnp.ndarray,\n",
    "        dropout_rng = None\n",
    "    ):\n",
    "        \n",
    "        attention_outputs = []\n",
    "        for head_i in range(self.num_heads):\n",
    "            single_head_attention = AttentionMechanism(d_key= self.d_model // self.num_heads, d_value = self.d_model // self.num_heads, dropout_rate=self.dropout_rate)(sequence, target_sequence, dropout_rng)\n",
    "            attention_outputs.append(single_head_attention)\n",
    "        attention_outputs = jnp.concatenate(attention_outputs, axis=1)\n",
    "        # Final projection of concatenated outputs\n",
    "        return nn.Dense(features=self.d_model)(attention_outputs) # (NQ x self.d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[ 0.09218917,  1.021213  ,  0.38755322, -1.438241  ,  0.32310086,\n",
       "        -0.5021071 ,  1.367448  ,  0.3488834 , -0.04968318,  1.2304572 ,\n",
       "        -0.5584813 , -0.1837434 ,  1.2924222 ,  0.67134094,  0.86011845,\n",
       "         1.0973927 ,  1.7717434 ,  0.25737727,  0.04999864, -0.06061977,\n",
       "         0.12868129, -1.2585343 ,  0.25587434,  1.3699903 , -0.21463393,\n",
       "        -0.59999275, -1.5419455 ,  0.14009482,  0.5536306 ,  0.3260527 ,\n",
       "        -0.6475275 ,  0.336717  ],\n",
       "       [ 0.09218917,  1.021213  ,  0.38755322, -1.438241  ,  0.32310086,\n",
       "        -0.5021071 ,  1.367448  ,  0.3488834 , -0.04968318,  1.2304572 ,\n",
       "        -0.5584813 , -0.1837434 ,  1.2924222 ,  0.67134094,  0.86011845,\n",
       "         1.0973927 ,  1.7717434 ,  0.25737727,  0.04999864, -0.06061977,\n",
       "         0.12868129, -1.2585343 ,  0.25587434,  1.3699903 , -0.21463393,\n",
       "        -0.59999275, -1.5419455 ,  0.14009482,  0.5536306 ,  0.3260527 ,\n",
       "        -0.6475275 ,  0.336717  ],\n",
       "       [ 0.09218917,  1.021213  ,  0.38755322, -1.438241  ,  0.32310086,\n",
       "        -0.5021071 ,  1.367448  ,  0.3488834 , -0.04968318,  1.2304572 ,\n",
       "        -0.5584813 , -0.1837434 ,  1.2924222 ,  0.67134094,  0.86011845,\n",
       "         1.0973927 ,  1.7717434 ,  0.25737727,  0.04999864, -0.06061977,\n",
       "         0.12868129, -1.2585343 ,  0.25587434,  1.3699903 , -0.21463393,\n",
       "        -0.59999275, -1.5419455 ,  0.14009482,  0.5536306 ,  0.3260527 ,\n",
       "        -0.6475275 ,  0.336717  ],\n",
       "       [ 0.09218917,  1.021213  ,  0.38755322, -1.438241  ,  0.32310086,\n",
       "        -0.5021071 ,  1.367448  ,  0.3488834 , -0.04968318,  1.2304572 ,\n",
       "        -0.5584813 , -0.1837434 ,  1.2924222 ,  0.67134094,  0.86011845,\n",
       "         1.0973927 ,  1.7717434 ,  0.25737727,  0.04999864, -0.06061977,\n",
       "         0.12868129, -1.2585343 ,  0.25587434,  1.3699903 , -0.21463393,\n",
       "        -0.59999275, -1.5419455 ,  0.14009482,  0.5536306 ,  0.3260527 ,\n",
       "        -0.6475275 ,  0.336717  ],\n",
       "       [ 0.09218917,  1.021213  ,  0.38755322, -1.438241  ,  0.32310086,\n",
       "        -0.5021071 ,  1.367448  ,  0.3488834 , -0.04968318,  1.2304572 ,\n",
       "        -0.5584813 , -0.1837434 ,  1.2924222 ,  0.67134094,  0.86011845,\n",
       "         1.0973927 ,  1.7717434 ,  0.25737727,  0.04999864, -0.06061977,\n",
       "         0.12868129, -1.2585343 ,  0.25587434,  1.3699903 , -0.21463393,\n",
       "        -0.59999275, -1.5419455 ,  0.14009482,  0.5536306 ,  0.3260527 ,\n",
       "        -0.6475275 ,  0.336717  ],\n",
       "       [ 0.09218917,  1.021213  ,  0.38755322, -1.438241  ,  0.32310086,\n",
       "        -0.5021071 ,  1.367448  ,  0.3488834 , -0.04968318,  1.2304572 ,\n",
       "        -0.5584813 , -0.1837434 ,  1.2924222 ,  0.67134094,  0.86011845,\n",
       "         1.0973927 ,  1.7717434 ,  0.25737727,  0.04999864, -0.06061977,\n",
       "         0.12868129, -1.2585343 ,  0.25587434,  1.3699903 , -0.21463393,\n",
       "        -0.59999275, -1.5419455 ,  0.14009482,  0.5536306 ,  0.3260527 ,\n",
       "        -0.6475275 ,  0.336717  ],\n",
       "       [ 0.09218917,  1.021213  ,  0.38755322, -1.438241  ,  0.32310086,\n",
       "        -0.5021071 ,  1.367448  ,  0.3488834 , -0.04968318,  1.2304572 ,\n",
       "        -0.5584813 , -0.1837434 ,  1.2924222 ,  0.67134094,  0.86011845,\n",
       "         1.0973927 ,  1.7717434 ,  0.25737727,  0.04999864, -0.06061977,\n",
       "         0.12868129, -1.2585343 ,  0.25587434,  1.3699903 , -0.21463393,\n",
       "        -0.59999275, -1.5419455 ,  0.14009482,  0.5536306 ,  0.3260527 ,\n",
       "        -0.6475275 ,  0.336717  ],\n",
       "       [ 0.09218917,  1.021213  ,  0.38755322, -1.438241  ,  0.32310086,\n",
       "        -0.5021071 ,  1.367448  ,  0.3488834 , -0.04968318,  1.2304572 ,\n",
       "        -0.5584813 , -0.1837434 ,  1.2924222 ,  0.67134094,  0.86011845,\n",
       "         1.0973927 ,  1.7717434 ,  0.25737727,  0.04999864, -0.06061977,\n",
       "         0.12868129, -1.2585343 ,  0.25587434,  1.3699903 , -0.21463393,\n",
       "        -0.59999275, -1.5419455 ,  0.14009482,  0.5536306 ,  0.3260527 ,\n",
       "        -0.6475275 ,  0.336717  ],\n",
       "       [ 0.09218917,  1.021213  ,  0.38755322, -1.438241  ,  0.32310086,\n",
       "        -0.5021071 ,  1.367448  ,  0.3488834 , -0.04968318,  1.2304572 ,\n",
       "        -0.5584813 , -0.1837434 ,  1.2924222 ,  0.67134094,  0.86011845,\n",
       "         1.0973927 ,  1.7717434 ,  0.25737727,  0.04999864, -0.06061977,\n",
       "         0.12868129, -1.2585343 ,  0.25587434,  1.3699903 , -0.21463393,\n",
       "        -0.59999275, -1.5419455 ,  0.14009482,  0.5536306 ,  0.3260527 ,\n",
       "        -0.6475275 ,  0.336717  ],\n",
       "       [ 0.09218917,  1.021213  ,  0.38755322, -1.438241  ,  0.32310086,\n",
       "        -0.5021071 ,  1.367448  ,  0.3488834 , -0.04968318,  1.2304572 ,\n",
       "        -0.5584813 , -0.1837434 ,  1.2924222 ,  0.67134094,  0.86011845,\n",
       "         1.0973927 ,  1.7717434 ,  0.25737727,  0.04999864, -0.06061977,\n",
       "         0.12868129, -1.2585343 ,  0.25587434,  1.3699903 , -0.21463393,\n",
       "        -0.59999275, -1.5419455 ,  0.14009482,  0.5536306 ,  0.3260527 ,\n",
       "        -0.6475275 ,  0.336717  ]], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multi_head = MultiHeadedAttention(d_model = 32, num_heads=8)\n",
    "rng, init_rng = jax.random.split(jax.random.PRNGKey(42), 2)\n",
    "params = multi_head.init(init_rng, jnp.ones((10, 32)), jnp.ones((10, 32)))\n",
    "multi_head.apply(params, jnp.ones((10, 32)), jnp.ones((10, 32)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parallelism of Attention Mechanism\n",
    "\n",
    "In recurrent (autoregressive) architectures, we need to process the sequence sequentially as we require the internal state to be updates with the previous outputs/inputs to ensure our model is casual. However attention mechanisms calculuate attention to all tokens in the sequence at once, which can be made casual with masking, meaning we can input entire batches and process them at once. **Ultimately the attention mechanism, and therefore the transformer architecture, does not depend on sequential computation**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self Attention and Cross Attention\n",
    "\n",
    "Self-Attention and Cross-Attention refer to how we construct the key, query and value matrices in our model. In self-attention the keys, queries and values are all learned from the same sequence. The idea is to see how parts of the input sequence attend to different parts of the input sequence. This is used to learn dependencies between different parts of the input sequence.\n",
    "\n",
    "Cross-Attention is when the queries come from one sequence but the keys and values come from another. This is used to see how tokens from one sequence attend to tokens from another sequence, learning relationships between elements in the two sequences, and it is commonly used in the decoder part of a Transformer model in sequence-to-sequence tasks, such as machine translation, where the decoder learns how to attend to the encoder's output while generating a target sequence. \n",
    "\n",
    "### Machine Translation Example:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Architecture\n",
    "\n",
    "## Encoder Block\n",
    "With more attention blocks in series our embeddings become more and more contextualised.\n",
    "\n",
    "Our encoder block uses residual connections to help mitigate the vanishing gradient problem in our deep architecture, it improves the flow of gradients and makes the updates more substantial deep into the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "    d_model : int\n",
    "    num_heads : int\n",
    "    dropout_rate : float = 0.0\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self,\n",
    "        sequence : jnp.ndarray, # (NK=NQ, d_model) In the encoder, we only do self-attention, as such we only need one input sequence, which attends to itself\n",
    "        dropout_rng = None\n",
    "    ):\n",
    "        attention_output = MultiHeadedAttention(d_model=self.d_model, num_heads=self.num_heads, dropout_rate=self.dropout_rate)(sequence, sequence, dropout_rng) # (NK x d_model)\n",
    "        residual = nn.LayerNorm()(attention_output + sequence)\n",
    "        linear = nn.Dense(features = self.d_model)(residual)\n",
    "        return nn.LayerNorm()(residual + linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[-1.6010164 , -0.74460006, -0.37307504, -0.81092244,  0.15336768,\n",
       "        -0.06796968,  0.47183788,  0.26438254,  0.1073863 ,  1.5642092 ,\n",
       "        -0.00715235,  0.81226546, -0.55497384,  0.18102111, -2.0238996 ,\n",
       "         0.41321406, -0.19702047,  2.1614182 , -1.353574  ,  0.042718  ,\n",
       "        -1.0291138 , -0.6032628 ,  0.9482072 ,  0.69363105, -0.75029397,\n",
       "        -1.5485976 ,  0.68653333,  0.18166786, -0.06982882, -0.63098276,\n",
       "         1.7092276 ,  1.9751959 ],\n",
       "       [-1.6010164 , -0.74460006, -0.37307504, -0.81092244,  0.15336768,\n",
       "        -0.06796968,  0.47183788,  0.26438254,  0.1073863 ,  1.5642092 ,\n",
       "        -0.00715235,  0.81226546, -0.55497384,  0.18102111, -2.0238996 ,\n",
       "         0.41321406, -0.19702047,  2.1614182 , -1.353574  ,  0.042718  ,\n",
       "        -1.0291138 , -0.6032628 ,  0.9482072 ,  0.69363105, -0.75029397,\n",
       "        -1.5485976 ,  0.68653333,  0.18166786, -0.06982882, -0.63098276,\n",
       "         1.7092276 ,  1.9751959 ],\n",
       "       [-1.6010164 , -0.74460006, -0.37307504, -0.81092244,  0.15336768,\n",
       "        -0.06796968,  0.47183788,  0.26438254,  0.1073863 ,  1.5642092 ,\n",
       "        -0.00715235,  0.81226546, -0.55497384,  0.18102111, -2.0238996 ,\n",
       "         0.41321406, -0.19702047,  2.1614182 , -1.353574  ,  0.042718  ,\n",
       "        -1.0291138 , -0.6032628 ,  0.9482072 ,  0.69363105, -0.75029397,\n",
       "        -1.5485976 ,  0.68653333,  0.18166786, -0.06982882, -0.63098276,\n",
       "         1.7092276 ,  1.9751959 ],\n",
       "       [-1.6010164 , -0.74460006, -0.37307504, -0.81092244,  0.15336768,\n",
       "        -0.06796968,  0.47183788,  0.26438254,  0.1073863 ,  1.5642092 ,\n",
       "        -0.00715235,  0.81226546, -0.55497384,  0.18102111, -2.0238996 ,\n",
       "         0.41321406, -0.19702047,  2.1614182 , -1.353574  ,  0.042718  ,\n",
       "        -1.0291138 , -0.6032628 ,  0.9482072 ,  0.69363105, -0.75029397,\n",
       "        -1.5485976 ,  0.68653333,  0.18166786, -0.06982882, -0.63098276,\n",
       "         1.7092276 ,  1.9751959 ],\n",
       "       [-1.6010164 , -0.74460006, -0.37307504, -0.81092244,  0.15336768,\n",
       "        -0.06796968,  0.47183788,  0.26438254,  0.1073863 ,  1.5642092 ,\n",
       "        -0.00715235,  0.81226546, -0.55497384,  0.18102111, -2.0238996 ,\n",
       "         0.41321406, -0.19702047,  2.1614182 , -1.353574  ,  0.042718  ,\n",
       "        -1.0291138 , -0.6032628 ,  0.9482072 ,  0.69363105, -0.75029397,\n",
       "        -1.5485976 ,  0.68653333,  0.18166786, -0.06982882, -0.63098276,\n",
       "         1.7092276 ,  1.9751959 ],\n",
       "       [-1.6010164 , -0.74460006, -0.37307504, -0.81092244,  0.15336768,\n",
       "        -0.06796968,  0.47183788,  0.26438254,  0.1073863 ,  1.5642092 ,\n",
       "        -0.00715235,  0.81226546, -0.55497384,  0.18102111, -2.0238996 ,\n",
       "         0.41321406, -0.19702047,  2.1614182 , -1.353574  ,  0.042718  ,\n",
       "        -1.0291138 , -0.6032628 ,  0.9482072 ,  0.69363105, -0.75029397,\n",
       "        -1.5485976 ,  0.68653333,  0.18166786, -0.06982882, -0.63098276,\n",
       "         1.7092276 ,  1.9751959 ],\n",
       "       [-1.6010164 , -0.74460006, -0.37307504, -0.81092244,  0.15336768,\n",
       "        -0.06796968,  0.47183788,  0.26438254,  0.1073863 ,  1.5642092 ,\n",
       "        -0.00715235,  0.81226546, -0.55497384,  0.18102111, -2.0238996 ,\n",
       "         0.41321406, -0.19702047,  2.1614182 , -1.353574  ,  0.042718  ,\n",
       "        -1.0291138 , -0.6032628 ,  0.9482072 ,  0.69363105, -0.75029397,\n",
       "        -1.5485976 ,  0.68653333,  0.18166786, -0.06982882, -0.63098276,\n",
       "         1.7092276 ,  1.9751959 ],\n",
       "       [-1.6010164 , -0.74460006, -0.37307504, -0.81092244,  0.15336768,\n",
       "        -0.06796968,  0.47183788,  0.26438254,  0.1073863 ,  1.5642092 ,\n",
       "        -0.00715235,  0.81226546, -0.55497384,  0.18102111, -2.0238996 ,\n",
       "         0.41321406, -0.19702047,  2.1614182 , -1.353574  ,  0.042718  ,\n",
       "        -1.0291138 , -0.6032628 ,  0.9482072 ,  0.69363105, -0.75029397,\n",
       "        -1.5485976 ,  0.68653333,  0.18166786, -0.06982882, -0.63098276,\n",
       "         1.7092276 ,  1.9751959 ],\n",
       "       [-1.6010164 , -0.74460006, -0.37307504, -0.81092244,  0.15336768,\n",
       "        -0.06796968,  0.47183788,  0.26438254,  0.1073863 ,  1.5642092 ,\n",
       "        -0.00715235,  0.81226546, -0.55497384,  0.18102111, -2.0238996 ,\n",
       "         0.41321406, -0.19702047,  2.1614182 , -1.353574  ,  0.042718  ,\n",
       "        -1.0291138 , -0.6032628 ,  0.9482072 ,  0.69363105, -0.75029397,\n",
       "        -1.5485976 ,  0.68653333,  0.18166786, -0.06982882, -0.63098276,\n",
       "         1.7092276 ,  1.9751959 ],\n",
       "       [-1.6010164 , -0.74460006, -0.37307504, -0.81092244,  0.15336768,\n",
       "        -0.06796968,  0.47183788,  0.26438254,  0.1073863 ,  1.5642092 ,\n",
       "        -0.00715235,  0.81226546, -0.55497384,  0.18102111, -2.0238996 ,\n",
       "         0.41321406, -0.19702047,  2.1614182 , -1.353574  ,  0.042718  ,\n",
       "        -1.0291138 , -0.6032628 ,  0.9482072 ,  0.69363105, -0.75029397,\n",
       "        -1.5485976 ,  0.68653333,  0.18166786, -0.06982882, -0.63098276,\n",
       "         1.7092276 ,  1.9751959 ]], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_block = EncoderBlock(d_model = 32, num_heads = 8)\n",
    "rng, init_rng = jax.random.split(jax.random.PRNGKey(42), 2)\n",
    "params = encoder_block.init(init_rng, jnp.ones((10, 32)))\n",
    "encoder_block.apply(params, jnp.ones((10, 32)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attention Invariance and Positional Encoding\n",
    "\n",
    "Because the attention mechanism works via dot products across the entire key set, permuting the keys and associated values, makes no difference on the output. Furthermore permuting the queries will only permute the resulting attention matrix (`equivariance'). Therefore **attention models are oblivious to the relative positioning of tokens in the sequence**. This is highly suitable for stationary sequences - where the distribution of values is independent of position in the sequence. For many applications, such as natural language processing, the absolute positioning of tokens has important semantic and syntactic meaning, which necessitates positional encoding in our tokens.\n",
    "\n",
    "The position encoding described in the original transformer model is given by:\n",
    "- If feature_index is even $PE(position, feature\\_index) = PE(position, 2i) = \\sin(\\frac{pos}{10000^{2i/d_{model}}})$.\n",
    "\n",
    "- If feature_index is odd $PE(position, feature\\_index + 1) = PE(position, 2i + 1) = \\cos(\\frac{pos}{10000^{2i/d_{model}}})$.\n",
    "\n",
    "This is used as there is a linear relationship between the positional encodings, so they are easy to learn. The very low frequency means it is unlikely to repeat and multiple tokens don't have the same positional encoding.\n",
    "\n",
    "$$ \\sin(a + b) = \\sin(a)\\cos(b) + \\sin(b)\\cos(a) $$\n",
    "$$ PE(k + n, 2i) = \\sin(\\frac{k}{\\omega} + \\frac{n}{\\omega}) = \\sin(\\frac{k}{\\omega})\\cos(\\frac{n}{\\omega}) + \\sin(\\frac{n}{\\omega})\\cos(\\frac{k}{\\omega}) = PE(k, 2i)w_{1} + PE(n, 2i)w_{2}. $$\n",
    "\n",
    "Positional Encoding is done before query/keys are constructed and done to the initial token embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoder(nn.Module):\n",
    "    num_data : int\n",
    "    num_features : int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self):\n",
    "        frequencies = jnp.array([[pos/(10_000 ** (2 * feat / self.num_features)) for feat in range(self.num_features)] for pos in range(self.num_data)])\n",
    "        even_positions = jnp.sin(frequencies)\n",
    "        odd_positions = jnp.cos(frequencies)\n",
    "        positional_encodings = even_positions\n",
    "        positional_encodings.at[:, 1::2].set(odd_positions[:, 1::2])\n",
    "        return positional_encodings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the encoder, the only attention is self-attention over the input sequence, here our model learns patterns in the input sequence. Additionally, we need to project the data to the model dimensionality first and then add our positional encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    d_model : int\n",
    "    num_heads : int\n",
    "    num_encoder_blocks : int\n",
    "    dropout_rate : float = 0.0\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self,\n",
    "        sequence : jnp.ndarray, # (NQ=NK, embedding_dimension)\n",
    "        dropout_rng = None\n",
    "    ):\n",
    "        project_sequence = nn.Dense(features=self.d_model)(sequence)\n",
    "        positional_encodings = PositionalEncoder(num_data = sequence.shape[0], num_features = self.d_model)()\n",
    "        project_sequence += positional_encodings\n",
    "        for block in range(self.num_encoder_blocks):\n",
    "            if dropout_rng is not None:\n",
    "                dropout_rng, block_rng = jax.random.split(dropout_rng, 2)\n",
    "                x = EncoderBlock(d_model = self.d_model, num_heads = self.num_heads, dropout_rate = self.dropout_rate)(project_sequence, block_rng)\n",
    "            else:\n",
    "                x = EncoderBlock(d_model = self.d_model, num_heads = self.num_heads, dropout_rate = self.dropout_rate)(project_sequence)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[ 0.2270196 , -0.88147086,  1.7586254 ,  0.61025095, -1.4935668 ,\n",
       "        -0.09663835, -0.25542971, -1.6849537 , -0.85669243,  1.7140261 ,\n",
       "         1.0995613 , -1.9956753 ,  0.37985593, -1.5971788 ,  0.01673846,\n",
       "         1.9369373 ,  0.7516392 , -0.56977063,  0.01491532, -0.5858998 ,\n",
       "        -0.39314574,  0.04456951,  0.47563976, -0.17209879, -0.85661393,\n",
       "         0.47149247, -0.1941591 ,  0.2547242 ,  1.9583863 , -0.23300745,\n",
       "        -0.49226698,  0.6441873 ],\n",
       "       [ 0.5688276 , -0.7960301 ,  1.6817492 ,  0.69646233, -1.405921  ,\n",
       "         0.48810703, -0.15471283, -1.7160313 , -0.9328121 ,  1.3078345 ,\n",
       "         1.1444054 , -1.9570963 , -0.01601211, -1.9795674 ,  0.17113669,\n",
       "         1.7458769 ,  0.727223  , -0.6600679 ,  0.1649453 , -0.7671649 ,\n",
       "        -0.01673924,  0.15837243,  0.5285803 , -0.65436304, -0.62370676,\n",
       "         0.1977588 ,  0.0137896 ,  0.3212364 ,  2.0461264 ,  0.21390757,\n",
       "        -0.8088062 ,  0.3126921 ],\n",
       "       [ 0.6432511 , -0.68558127,  1.6664013 ,  0.75914097, -1.2309873 ,\n",
       "         0.7305423 , -0.18225107, -1.5762525 , -0.9970239 ,  1.183226  ,\n",
       "         1.1054218 , -1.9556679 , -0.18490705, -2.1545749 ,  0.18678087,\n",
       "         1.6619879 ,  0.67175204, -0.55608827,  0.2708189 , -0.82520014,\n",
       "         0.09779699,  0.11451466,  0.46537626, -0.8520419 , -0.63369966,\n",
       "         0.22015344,  0.12060174,  0.28724217,  2.055615  ,  0.31141642,\n",
       "        -0.9397484 ,  0.22198424],\n",
       "       [ 0.54696727, -0.6293441 ,  1.777179  ,  0.8238267 , -0.9885712 ,\n",
       "         0.595268  , -0.3608941 , -1.3506436 , -1.1023628 ,  1.3601956 ,\n",
       "         0.99545044, -2.0472438 , -0.09241188, -2.1999764 ,  0.04648354,\n",
       "         1.6764829 ,  0.61474377, -0.19619387,  0.3495152 , -0.79509836,\n",
       "        -0.0696954 , -0.13495721,  0.27085504, -0.7360258 , -0.9916604 ,\n",
       "         0.5558134 ,  0.07528933,  0.16595662,  2.0677361 ,  0.07291164,\n",
       "        -0.78727686,  0.48768154],\n",
       "       [ 0.50908166, -0.6472931 ,  1.8435678 ,  0.8673967 , -0.6699304 ,\n",
       "         0.29135656, -0.5462767 , -1.1554796 , -1.1564567 ,  1.4958092 ,\n",
       "         0.8808623 , -2.0268307 ,  0.11741099, -2.085106  , -0.15548986,\n",
       "         1.5650194 ,  0.55603987,  0.29688096,  0.40309486, -0.69870055,\n",
       "        -0.30419663, -0.44718298,  0.05454706, -0.52177894, -1.4406229 ,\n",
       "         0.92777485, -0.04933559,  0.03861557,  2.0379386 , -0.2950869 ,\n",
       "        -0.5001794 ,  0.8145512 ],\n",
       "       [ 0.56903946, -0.8216852 ,  1.9670274 ,  0.9881939 , -0.5527717 ,\n",
       "         0.14697166, -0.5483417 , -1.2725062 , -1.1202359 ,  1.3665942 ,\n",
       "         0.8917539 , -1.9161644 ,  0.16579294, -1.8248928 , -0.27615795,\n",
       "         1.4793146 ,  0.5224478 ,  0.41778216,  0.3620562 , -0.717758  ,\n",
       "        -0.4248269 , -0.54486734,  0.03024583, -0.40527588, -1.4994708 ,\n",
       "         1.0851958 , -0.0822512 ,  0.06822171,  2.1481125 , -0.4289512 ,\n",
       "        -0.4772294 ,  0.7046362 ],\n",
       "       [ 0.66841155, -1.1203306 ,  2.161592  ,  1.1425635 , -0.6758234 ,\n",
       "         0.21379325, -0.34948364, -1.6512697 , -1.0164393 ,  1.0191647 ,\n",
       "         1.0331984 , -1.7363967 ,  0.03451838, -1.5103215 , -0.27904847,\n",
       "         1.484984  ,  0.5315459 ,  0.06955665,  0.20535183, -0.8578976 ,\n",
       "        -0.4083983 , -0.3711147 ,  0.22135498, -0.40860826, -1.1303718 ,\n",
       "         0.9819718 , -0.02853491,  0.23255493,  2.3874204 , -0.2764794 ,\n",
       "        -0.7468722 ,  0.1794089 ],\n",
       "       [ 0.77270925, -1.3298304 ,  2.2026458 ,  1.1474528 , -0.79166055,\n",
       "         0.31221423, -0.11278223, -1.9254069 , -0.8825817 ,  0.6995491 ,\n",
       "         1.1842961 , -1.484688  , -0.05736426, -1.302288  , -0.23656067,\n",
       "         1.4316165 ,  0.5721767 , -0.30068094,  0.0548744 , -0.918969  ,\n",
       "        -0.3163527 , -0.11316085,  0.4418564 , -0.47442666, -0.7666943 ,\n",
       "         0.74380547, -0.02543746,  0.34519893,  2.49712   , -0.11697732,\n",
       "        -0.9802427 , -0.26941094],\n",
       "       [ 0.751765  , -1.3863198 ,  2.1742373 ,  1.0886238 , -0.852617  ,\n",
       "         0.28843474, -0.01062415, -2.012642  , -0.7859785 ,  0.674075  ,\n",
       "         1.254371  , -1.362086  , -0.01108037, -1.2064545 , -0.26799348,\n",
       "         1.4502084 ,  0.6082615 , -0.44214538,  0.0039926 , -0.8933898 ,\n",
       "        -0.32014346,  0.02644529,  0.5495979 , -0.4448917 , -0.6893655 ,\n",
       "         0.6787271 , -0.09289456,  0.3563174 ,  2.5038466 , -0.16527276,\n",
       "        -1.0488254 , -0.41617966],\n",
       "       [ 0.5224462 , -1.2831806 ,  2.1416192 ,  1.0437559 , -0.9000987 ,\n",
       "         0.11143866, -0.08914253, -1.9362929 , -0.734317  ,  1.0054296 ,\n",
       "         1.213555  , -1.4371105 ,  0.16961114, -1.1608056 , -0.3925354 ,\n",
       "         1.6340888 ,  0.622956  , -0.37420326,  0.04904454, -0.814507  ,\n",
       "        -0.4829062 , -0.00443704,  0.5230431 , -0.24118704, -0.88149834,\n",
       "         0.8781568 , -0.20691682,  0.2819948 ,  2.428968  , -0.44807583,\n",
       "        -0.9648127 , -0.2740801 ]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder = Encoder(d_model = 32, num_heads = 8, num_encoder_blocks=6, dropout_rate=0.0)\n",
    "rng, init_rng = jax.random.split(jax.random.PRNGKey(42), 2)\n",
    "params = encoder.init(init_rng, jnp.ones((10, 128)))\n",
    "encoder.apply(params, jnp.ones((10, 128)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "\n",
    "Now let's understand and implement the decoder block. In the original transformer architecture, the decoder contains two attention mechanisms, a self-attention of the produced output sequence so far, to itself, which learns how the output embeddings relate to each other. And a cross-attention between the output tokens and the output of the encoder, which how tokens in our output sequence relate to the input sequence. Therefore, it still produces translations in a sequential, auto-regressive manner. In the cross-attention, the keys and values are from the english sequence and the queries from our german sequence, the output therefore is what the meaning of each word is, in the english context.\n",
    "\n",
    "In the decoder masking is used as we do not know the future predicted tokens, its used in inference for self attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "\n",
    "    d_model : int\n",
    "    num_heads : int\n",
    "    dropout_rate : float = 0.0\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self,\n",
    "        encoder_output,\n",
    "        output_sequence, \n",
    "        dropout_rng = None\n",
    "    ):\n",
    "        output_self_attention = MultiHeadedAttention(d_model=self.d_model, num_heads=self.num_heads, dropout_rate=self.dropout_rate)(output_sequence, output_sequence, dropout_rng) # (NK x d_model)\n",
    "        residual = nn.LayerNorm()(output_self_attention + output_sequence)\n",
    "        \n",
    "        cross_attention = MultiHeadedAttention(d_model=self.d_model, num_heads=self.num_heads, dropout_rate=self.dropout_rate)(encoder_output, output_self_attention, dropout_rng)\n",
    "        residual = nn.LayerNorm()(cross_attention + residual)\n",
    "\n",
    "        linear = nn.Dense(features = self.d_model)(residual)\n",
    "        return nn.LayerNorm()(residual + linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[ 0.8705498 , -0.29904237,  1.065743  , -0.92596537,  0.8594881 ,\n",
       "        -0.06898125,  0.43973774, -0.40434396,  0.88021624, -0.5962909 ,\n",
       "        -1.3918655 ,  2.4014132 ,  0.4867272 ,  0.6781453 ,  1.2331144 ,\n",
       "        -2.0076938 , -0.03978869,  0.5281033 , -0.2670866 ,  1.6258852 ,\n",
       "        -1.3171986 ,  0.91133046,  1.0324557 , -1.4080709 , -1.2728466 ,\n",
       "        -0.5505679 , -0.05560639, -0.73855853,  0.5119102 , -0.9049106 ,\n",
       "        -0.6274499 , -0.64855194]], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_block = DecoderBlock(d_model = 32, num_heads=8, dropout_rate=0.0)\n",
    "rng, init_rng = jax.random.split(jax.random.PRNGKey(42), 2)\n",
    "params = decoder_block.init(init_rng, jnp.ones((1, 32)), jnp.ones((10, 32)))\n",
    "decoder_block.apply(params, jnp.ones((10, 32)), jnp.ones((1, 32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    d_model : int\n",
    "    num_heads : int\n",
    "    num_decoder_blocks : int\n",
    "    dropout_rate : float = 0.0\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self,\n",
    "        encoder_output,\n",
    "        output_sequence,\n",
    "        dropout_rng = None\n",
    "    ):\n",
    "        projected_output = nn.Dense(features=self.d_model)(output_sequence)\n",
    "        positional_encodings = PositionalEncoder(num_data=output_sequence.shape[0], num_features=self.d_model)()\n",
    "        projected_output += positional_encodings\n",
    "        x = projected_output\n",
    "        for decoder_block in range(self.num_decoder_blocks):\n",
    "            if (dropout_rng is not None and self.dropout_rate != 0.0):\n",
    "                dropout_rng, block_rng = jax.random.split(dropout_rng, 2)\n",
    "                x = DecoderBlock(d_model=self.d_model, num_heads=self.num_heads, dropout_rate=self.dropout_rate)(encoder_output, x, block_rng)\n",
    "            else:\n",
    "                x = DecoderBlock(d_model=self.d_model, num_heads=self.num_heads, dropout_rate=self.dropout_rate)(encoder_output, x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[-2.46774   ,  0.30575985, -1.2096788 , -0.01589864, -1.4317238 ,\n",
       "         0.15191452,  0.59750885,  1.4253247 , -0.97455406,  1.2900374 ,\n",
       "        -0.16072749,  1.1411221 ,  0.58991116,  0.14383507,  1.805051  ,\n",
       "        -0.24406984,  1.091023  , -0.10063185,  1.3763161 ,  0.39037886,\n",
       "        -0.41555667,  0.45323968,  0.4349356 , -1.3794247 ,  0.06797572,\n",
       "         0.2378329 , -0.25623277,  1.2141205 , -1.3303334 , -0.64375865,\n",
       "        -1.5839705 , -0.5019853 ],\n",
       "       [-2.6987092 , -0.50889957, -0.7936967 ,  1.5718756 ,  0.22204833,\n",
       "         0.21450977,  0.7557413 ,  1.8009727 , -0.7372998 ,  0.48573074,\n",
       "         1.0401368 ,  0.95349705,  0.11769154,  0.73372597, -0.47389382,\n",
       "        -0.4456925 , -1.3872129 , -0.9461556 ,  1.128169  , -1.0824682 ,\n",
       "         0.99234533,  0.8792735 ,  0.1793544 , -1.1905773 , -0.37299743,\n",
       "         0.80755424, -0.20205463, -0.2735211 ,  1.4139537 , -1.2048684 ,\n",
       "        -0.04597928, -0.9325538 ]], dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder = Decoder(d_model=32, num_heads=8, num_decoder_blocks=6, dropout_rate=0.0)\n",
    "rng, init_rng = jax.random.split(jax.random.PRNGKey(42), 2)\n",
    "params = decoder.init(init_rng, jnp.ones((10, 32)), jnp.ones((2, 32)))\n",
    "decoder.apply(params, jnp.ones((10, 32)), jnp.ones((2, 32)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    d_model : int\n",
    "    num_heads : int\n",
    "    num_encoder_blocks : int\n",
    "    num_decoder_blocks : int\n",
    "    dropout_rate : float\n",
    "    output_dim : int\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self,\n",
    "        input_sequence,\n",
    "        output_sequence,\n",
    "        dropout_rng = None\n",
    "    ):\n",
    "        encoder_output = Encoder(d_model=self.d_model, num_encoder_blocks=self.num_encoder_blocks, num_heads=self.num_heads, dropout_rate=self.dropout_rate)(input_sequence)\n",
    "        decoder_output = Decoder(d_model=self.d_model, num_decoder_blocks=self.num_decoder_blocks, num_heads=self.num_heads, dropout_rate=self.dropout_rate)(encoder_output, output_sequence)\n",
    "        return nn.softmax(nn.Dense(features=self.output_dim)(decoder_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([0.03418443, 0.03028339, 0.03722412, 0.02091815, 0.02253244,\n",
       "       0.03425919, 0.03262088, 0.06958006, 0.04557904, 0.28960752,\n",
       "       0.02728707, 0.15572648, 0.09763368, 0.00633427, 0.03984687,\n",
       "       0.05638238], dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer = Transformer(d_model=128, num_heads=8, num_encoder_blocks=6, num_decoder_blocks=6, dropout_rate=0.0, output_dim=16)\n",
    "rng, init_rng = jax.random.split(jax.random.PRNGKey(42), 2)\n",
    "input_sequence_encodings = jnp.ones((10, 12))\n",
    "output_generated_so_far = jnp.ones((2, 16))\n",
    "prediction_token = jnp.zeros((1, 16))\n",
    "output_generated_so_far = jnp.concatenate([output_generated_so_far, prediction_token], axis=0)\n",
    "params = transformer.init(init_rng, input_sequence_encodings, output_generated_so_far)\n",
    "transformer.apply(params, input_sequence_encodings, output_generated_so_far)[-1, :]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pinn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
