{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\textbf{Task: Create a simple regression model and fit using jax optimisers}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax.numpy as jnp\n",
    "import jax\n",
    "\n",
    "rng_key = jax.random.PRNGKey(42)\n",
    "rng_key, data_rng, true_rng, noise_rng = jax.random.split(rng_key, 4)\n",
    "\n",
    "N = 128\n",
    "\n",
    "xs = jax.random.normal(data_rng, shape=(N, 10))\n",
    "xs = jnp.concatenate([xs, jnp.ones((N, 1))], axis=1)\n",
    "true_beta = (jax.random.uniform(true_rng, shape=(11, 1)) * 20) - 10\n",
    "ys = xs @ true_beta + jax.random.normal(noise_rng, shape=(N, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optax\n",
    "from jax.nn.initializers import lecun_uniform\n",
    "\n",
    "rng_key, rng_init = jax.random.split(rng_key, 2)\n",
    "\n",
    "initialiser = lecun_uniform()\n",
    "weights = initialiser(rng_init, (11, 1), dtype=jnp.float32)\n",
    "\n",
    "learning_rate = 5e-2\n",
    "optimiser = optax.adam(learning_rate)\n",
    "optimiser_state = optimiser.init(weights)\n",
    "\n",
    "@jax.jit\n",
    "def fwd(weights, batch_in):\n",
    "    return batch_in @ weights\n",
    "\n",
    "@jax.jit\n",
    "def mse_loss(weights, batch_in, batch_out):\n",
    "    return jnp.mean(jnp.square(fwd(weights, batch_in) - batch_out)) + jnp.mean(jnp.square(weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199.41626\n",
      "115.69794\n",
      "68.3955\n",
      "39.97103\n",
      "33.384377\n",
      "29.50718\n",
      "25.952251\n",
      "25.229612\n",
      "24.513538\n",
      "24.743563\n",
      "True beta: [[ 1.2040024 ]\n",
      " [ 9.90509   ]\n",
      " [ 3.9389248 ]\n",
      " [-0.37490082]\n",
      " [ 7.4107857 ]\n",
      " [-4.6295214 ]\n",
      " [ 2.2635794 ]\n",
      " [ 5.3508472 ]\n",
      " [ 7.32815   ]\n",
      " [ 2.3363113 ]\n",
      " [-1.0534973 ]]\n",
      "Learned beta: [[ 1.084684  ]\n",
      " [ 8.980898  ]\n",
      " [ 3.7051468 ]\n",
      " [-0.28055766]\n",
      " [ 6.8415504 ]\n",
      " [-4.022745  ]\n",
      " [ 1.7459644 ]\n",
      " [ 4.877995  ]\n",
      " [ 6.72596   ]\n",
      " [ 1.7789909 ]\n",
      " [-0.94609976]]\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "batch_size = 32\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    batch_losses = []\n",
    "    for batch in range(N // batch_size):\n",
    "        rng_key, rng_batch = jax.random.split(rng_key, 2)\n",
    "        batch_indices = jax.random.choice(rng_batch, N, shape=(batch_size, ), replace=False)\n",
    "        batch_in = xs[batch_indices, :]\n",
    "        batch_out = ys[batch_indices]\n",
    "        loss, grads = jax.value_and_grad(mse_loss, argnums=(0))(weights, batch_in, batch_out)\n",
    "        updates, optimiser_state = optimiser.update(grads, optimiser_state)\n",
    "        weights = optax.apply_updates(weights, updates)\n",
    "        batch_losses.append(loss)\n",
    "    if ((epoch + 1) % 10 == 0):\n",
    "        print(jnp.array(batch_losses).mean())\n",
    "        \n",
    "print(f\"True beta: {true_beta}\")\n",
    "print(f\"Learned beta: {weights}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pinn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
