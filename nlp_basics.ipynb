{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Basics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/peter/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenising"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I am learning natural language processing.', \"Peter's conda is not working.\"]\n",
      "['I', 'am', 'learning', 'natural', 'language', 'processing', '.', 'Peter', \"'s\", 'conda', 'is', 'not', 'working', '.']\n",
      "['i', 'am', 'learning', 'natural', 'language', 'processing', '.', 'peter', \"'s\", 'conda', 'is', 'not', 'working', '.']\n"
     ]
    }
   ],
   "source": [
    "text = \"I am learning natural language processing. Peter's conda is not working.\"\n",
    "\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "print(sentences)\n",
    "tokens = nltk.word_tokenize(text)\n",
    "print(tokens)\n",
    "\n",
    "normalised_tokens = [token.lower() for token in tokens]\n",
    "print(normalised_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming\n",
    "\n",
    "Stemming is the process of reducing words to a root-form, usually by removing suffixes. Stemming is done to remove the vocabulary size, as words which stem the same have very similar meanings (small semantic differences) big semantic differences. For understanding context/information retrival, stemming makes sense, for understanding the language semantics, it does not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fight'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "word = \"fighting\"\n",
    "ps.stem(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('I', 'PRP'), ('am', 'VBP'), ('learning', 'VBG'), ('natural', 'JJ'), ('language', 'NN'), ('processing', 'NN'), ('.', '.'), ('Peter', 'NNP'), (\"'s\", 'POS'), ('conda', 'NN'), ('is', 'VBZ'), ('not', 'RB'), ('working', 'VBG'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "tagged_tokens = nltk.pos_tag(tokens)\n",
    "print(tagged_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmaisation\n",
    "\n",
    "Lemmaisation reduces words to their dictionary root form. Lemmatisation requires POS-tagging to reduce to base form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'am', 'learning', 'natural', 'language', 'processing', '.', 'Peter', \"'s\", 'conda', 'is', 'not', 'working', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    return wordnet.NOUN\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "lemmatized_tokens = []\n",
    "for token, tag in tagged_tokens:\n",
    "    wn_tag = get_wordnet_pos(tag)\n",
    "    lemmatized_tokens.append(lemmatizer.lemmatize(token, pos=wn_tag))\n",
    "\n",
    "print(lemmatised_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  I/PRP\n",
      "  am/VBP\n",
      "  learning/VBG\n",
      "  natural/JJ\n",
      "  language/NN\n",
      "  processing/NN\n",
      "  ./.\n",
      "  (PERSON Peter/NNP)\n",
      "  's/POS\n",
      "  conda/NN\n",
      "  is/VBZ\n",
      "  not/RB\n",
      "  working/VBG\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "entities = nltk.chunk.ne_chunk(tagged)\n",
    "print(entities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Word Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.', 'processing', 'language', 'peter', 'learning', \"'s\", 'working', 'natural', 'conda']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords = set(stopwords.words('english'))\n",
    "normalised_tokens = set(normalised_tokens) - stopwords\n",
    "print(list(normalised_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
