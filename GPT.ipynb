{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nEaveLOxXm38",
        "outputId": "b28d5e25-978a-47e2-c97c-82ffc50d109a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-02-24 00:23:26--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8000::154, 2606:50c0:8001::154, 2606:50c0:8002::154, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8000::154|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.08s   \n",
            "\n",
            "2024-02-24 00:23:26 (13.1 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPragFHTXyE6",
        "outputId": "ebb0d66c-bb24-4cff-c70f-c281306a7ddf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You\n"
          ]
        }
      ],
      "source": [
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "  text = f.read()\n",
        "\n",
        "print(text[:100])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRgaB6HNYYP2"
      },
      "source": [
        "# Character Level Tokenisation\n",
        "\n",
        "We build a simple encoder, treating each character as a token, our tokens are the indexes of our character vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xXMdpEMIYdV4",
        "outputId": "30d01911-1380-46ab-b226-e5ca84560a53"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary size: 65\n",
            "Vocabulary: ['I', 'w', 'o', '.', 'v', 'M', 's', ';', 'j', 'X', '3', 'y', 'u', 'S', 'U', ':', 'e', ' ', 'E', 'K', '?', 'H', \"'\", 'F', 'A', 'J', 'C', 'r', ',', '\\n', 'n', 'q', 'f', 'a', 'i', '-', 'O', 'k', 'P', 'Q', '$', 'g', 'd', 'm', 'G', 't', 'p', '&', 'h', 'V', 'x', 'T', 'Y', 'R', 'B', '!', 'c', 'l', 'N', 'D', 'z', 'L', 'b', 'Z', 'W']\n"
          ]
        }
      ],
      "source": [
        "character_vocab = list(set(text))\n",
        "print(f\"Vocabulary size: {len(character_vocab)}\\nVocabulary: {character_vocab}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZSW1zbxY_TN"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0wPesA7pY861",
        "outputId": "97b5d42d-993d-4f88-ff60-799d690b22cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First Citizen:\n",
            "Befor\n",
            "[23, 34, 27, 6, 45, 17, 26, 34, 45, 34, 60, 16, 30, 15, 29, 54, 16, 32, 2, 27]\n",
            "['F', 'i', 'r', 's', 't', ' ', 'C', 'i', 't', 'i', 'z', 'e', 'n', ':', '\\n', 'B', 'e', 'f', 'o', 'r']\n"
          ]
        }
      ],
      "source": [
        "stoi = {ch:i for i, ch in enumerate(character_vocab)}\n",
        "itos = {i:ch for ch, i in stoi.items()}\n",
        "\n",
        "encode = lambda s : [stoi[c] for c in s]\n",
        "decode = lambda ts : [itos[t] for t in ts]\n",
        "\n",
        "print(text[:20])\n",
        "print(encode(text[:20]))\n",
        "print(decode(encode(text[:20])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dm_vp2Q3akc8",
        "outputId": "21a0489d-91a2-4455-8450-70fe3ee7efee"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_8417/2596755637.py:4: UserWarning: Explicitly requested dtype <class 'jax.numpy.float64'> requested in array is not available, and will be truncated to dtype float32. To enable more dtypes, set the jax_enable_x64 configuration option or the JAX_ENABLE_X64 shell environment variable. See https://github.com/google/jax#current-gotchas for more.\n",
            "  data = jnp.array(encode(text), dtype=jnp.float64)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[23. 34. 27.  6. 45. 17. 26. 34. 45. 34.]\n"
          ]
        }
      ],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "\n",
        "data = jnp.array(encode(text), dtype=jnp.float64)\n",
        "print(data[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "NlTcnrZdb3P3"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "train_data, test_data = train_test_split(data, train_size=int(0.9 * train_data.shape[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Eh4g3Wgdu54"
      },
      "source": [
        "# Training\n",
        "\n",
        "We don't pass the entire corpus in at once, instead we train the transformer on randomly sampled chunks of the data. The size of a chunk is given by the $\\texttt{block\\_size}$. From a single block we have many training examples, of all of the cumulative contexts so far in the block, this makes our model comfortable with dealing with contexts of variable size. After block_size we have to start truncating, as our model cannot extend beyond this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "xz-oT1I9d8dR"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "With an input context of [29.] the output is 57.0\n",
            "With an input context of [29. 57.] the output is 16.0\n",
            "With an input context of [29. 57. 16.] the output is 2.0\n",
            "With an input context of [29. 57. 16.  2.] the output is 6.0\n",
            "With an input context of [29. 57. 16.  2.  6.] the output is 17.0\n",
            "With an input context of [29. 57. 16.  2.  6. 17.] the output is 16.0\n",
            "With an input context of [29. 57. 16.  2.  6. 17. 16.] the output is 12.0\n",
            "With an input context of [29. 57. 16.  2.  6. 17. 16. 12.] the output is 33.0\n"
          ]
        }
      ],
      "source": [
        "block_size = 8\n",
        "\n",
        "example_block = train_data[:block_size + 1]\n",
        "for i in range(block_size):\n",
        "  print(f\"With an input context of {example_block[:i+1]} the output is {example_block[i+1]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_batch(data, batch_size = 128, sample_rng):\n",
        "    batch_indices = jnp.random.choice(sample_rng, n = batch_size, a = jnp.arange(data.shape[0]))\n",
        "    batch_inputs = np.array([])\n",
        "    batch_inputs = "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[8, 10, 9]\n"
          ]
        }
      ],
      "source": [
        "import heapq\n",
        "\n",
        "heap = []\n",
        "\n",
        "heapq.heappush(heap, 10)\n",
        "heapq.heappush(heap, 9)\n",
        "heapq.heappush(heap, 8)\n",
        "\n",
        "print(heap)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
