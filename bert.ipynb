{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **BERT** - **B**i-directional **E**ncoder **R**epresentations from **T**ransformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT is an encoder only (self-attention), large language model developed by Google in 2018. \n",
    "\n",
    "BERT is trains via self-supervised learning. Self-supervised: The model creates an artificial supervised learning task for training, based on the structure of the input data without explicit external labels. Contrasting with decoder architectures (such as ChatGPT), BERT does bi-directional attention, that is there is no masking in the attention mechanisms. This is common in making the attented weights causal, meaning that tokens can only attend to previous tokens in the sequence. \n",
    "\n",
    "The designers of BERT reasoned that for sentence level task it is crucial to incorporate context from both directions. This is distinct from causal (L -> R), backwards (R -> L) and combinations of separate independent (L -> R) and (R -> L) models. \n",
    "\n",
    "## Pre-training\n",
    "\n",
    "BERT is pretrained on two self-supervised training objectives. Firstly, is masked language modelling. $15\\%$ of the input tokens are masked. In pre-training, a masked token is given the following value: untouched as a masked token ($80\\%$), random token ($10\\%$) or the same token ($10\\%$). The reasoning for this split is that there are no-masking tokens in fine-tuning, it merely exists in pre-training, creating a mismatch between pre-training and fine-tuning. The model attempts to predict the true token behind the mask, from the other tokens in the sentence. \n",
    "\n",
    "The purpose of MLM is to understand the impact context has in determining other tokens in the sequence.\n",
    "\n",
    "Next sentence prediction. Given two sentences, input as one sequence separated by a \\[sep\\] token, the model tries to classify whether the second sentence plausibly follows the first. This objective aims to understand the relationship between two sentences, which is relevant to many NLP tasks, such as question answering, where we want the answer to be relevant to the question, of course.\n",
    "\n",
    "Both these pre-training objectives have different outputs, so they have a different final layer, however the rest of the model is the same underlying architecture. This highlights the flexibility of BERTs architecture, in that we do not need substantial architectual changes for different tasks, after pre-training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT Architecture\n",
    "\n",
    "BERT was presented in two forms, BASE and LARGE.\n",
    "\n",
    "| Variant | Number of Layers | $d_{model}$ | Multi-head Attention Heads | Total Parameters |\n",
    "|---------|------------------|-----------------------|----------------------------|------------------|\n",
    "| BASE    | 12               | 768  ($~=256 \\cdot 3$)  | 12 ($~=4\\cdot3$)             | 110M             |\n",
    "| LARGE   | 24               | 1024 ($~=254 \\cdot 4$)  | 16 ($~=4\\cdot4$)             | 340M             |\n",
    "\n",
    "Because BERT is bi-directional, there is no masking in the attention heads, tokens can even attend to themselves, to learn context across the whole sentence, both ways. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **GLUE**: General Language Understanding Evaluation\n",
    "\n",
    "GLUE are $11$ natural language understanding tasks.\n",
    "\n",
    "1. **CoLA (Corpus of Linguistic Acceptability):** Evaluates the ability to determine whether an English sentence is grammatically correct.\n",
    "2. **SST-2 (Stanford Sentiment Treebank):** Assesses the ability to determine the sentiment of a sentence (positive or negative).\n",
    "3. **MRPC (Microsoft Research Paraphrase Corpus):** Tests the ability to identify whether two sentences are semantically equivalent (paraphrases).\n",
    "4. **STS-B (Semantic Textual Similarity Benchmark):** Measures the ability to determine the similarity of two sentences on a continuous scale.\n",
    "5. **QQP (Quora Question Pairs):** Evaluates the ability to determine whether two questions asked on Quora are semantically equivalent.\n",
    "6. **MNLI (Multi-Genre Natural Language Inference):** Assesses the ability to determine the relationship between a pair of sentences (entailment, contradiction, or neutral).\n",
    "7. **QNLI (Question Natural Language Inference):** Tests the ability to determine whether a context sentence contains the answer to a question.\n",
    "8. **RTE (Recognizing Textual Entailment):** Evaluates the ability to determine whether one sentence entails another.\n",
    "9. **WNLI (Winograd Natural Language Inference):** Assesses the ability to resolve coreference and infer the relationship between two sentences.\n",
    "10. **AX (Diagnostic Set):** A diagnostic set designed to evaluate model performance on a diverse set of linguistic phenomena.\n",
    "11. **Diag (GLUE Diagnostic):** A diagnostic test set that is used to evaluate a model's understanding of various linguistic phenomena.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
